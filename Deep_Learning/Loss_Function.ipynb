{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44719975",
   "metadata": {},
   "source": [
    "## 均方误差 MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250ae100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss:  0.2875000834465027\n",
      "gradient of y:  tensor([-0.2500,  0.2500,  0.0500,  0.4000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "y_pred = torch.tensor([2.5, 0.0, 2.1, 7.8], requires_grad=True)\n",
    "y_true = torch.tensor([3.0, -0.5, 2.0, 7.0])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(y_pred, y_true)\n",
    "print(\"MSE Loss: \", loss.item())\n",
    "\n",
    "loss.backward()\n",
    "print(\"gradient of y: \", y_pred.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2078fb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss:  0.2875000834465027\n",
      "gradient manual:  tensor([-0.2500,  0.2500,  0.0500,  0.4000], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 手写\n",
    "diff = y_pred - y_true\n",
    "loss = torch.mean(diff**2)\n",
    "\n",
    "print(\"MSE Loss: \", loss.item())\n",
    "\n",
    "n = y_pred.shape[0]\n",
    "grad_manual = (2.0/n) * (y_pred - y_true)\n",
    "\n",
    "print(\"gradient manual: \", grad_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506eda5e",
   "metadata": {},
   "source": [
    "## 交叉熵\n",
    "二分类 \n",
    "\n",
    "torch.BCELoss()   \n",
    "\n",
    "$Loss = - \\frac{1}{n}\\sum_{i = 1}^{n}[y_i\\log(\\hat{y_i}) + (1 - y_i)\\log(1 - \\hat{y_i})]$   \n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial y_i} = -\\frac{1}{n}(\\frac{y_i}{\\hat{y_i}} - \\frac{1 - y_i}{1 - \\hat{y_i}})$\n",
    "\n",
    "torch.BCEWithLogitsLoss()\n",
    "\n",
    "$Loss = - \\frac{1}{n}\\sum_{i = 1}^{n}[y_i \\log(\\sigma(x_i)) + (1 - y_i)\\log(1 - \\sigma(x_i))]$  \n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$  \n",
    "\n",
    "$\\frac{dy_i}{dx_i} = \\frac{e^{-x_i}}{(1 + e^{-x_i})^2} = \\hat{y_i}(1 - \\hat{y_i})$   \n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial x_i} = \\frac{\\partial Loss}{\\partial y_i} \\times \\frac{dy_i}{dx_i} = -\\frac{1}{n}(y_i - \\hat{y_i})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4abb0cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCELoss:  0.2642763555049896\n",
      "gradient:  tensor([-0.4990,  0.4335, -0.3784])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "probs = torch.tensor([0.668, 0.231, 0.881], requires_grad=True)\n",
    "labels = torch.tensor([1, 0, 1], dtype=torch.float32)\n",
    "criterion = nn.BCELoss()\n",
    "loss = criterion(probs, labels)\n",
    "\n",
    "print(\"BCELoss: \", loss.item())\n",
    "\n",
    "loss.backward()\n",
    "print(\"gradient: \", probs.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3190b6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross Entropy Loss: 0.2644655108451843\n",
      "logits 的梯度： tensor([-0.1106,  0.0772, -0.0397])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设模型输出的是 logits（未经过 Sigmoid）\n",
    "logits = torch.tensor([0.7, -1.2, 2.0], requires_grad=True)  # shape: [batch_size]\n",
    "labels = torch.tensor([1.0, 0.0, 1.0])  # shape: [batch_size]\n",
    "\n",
    "# 使用 BCEWithLogitsLoss（自动对 logits 做 Sigmoid）\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(\"Binary Cross Entropy Loss:\", loss.item())\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "print(\"logits 的梯度：\", logits.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff87346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.2642762362957001\n",
      "Gradient:  tensor([-0.4990,  0.4335, -0.3784], grad_fn=<DivBackward0>)\n",
      "Autograd gradient: tensor([-0.4990,  0.4335, -0.3784])\n"
     ]
    }
   ],
   "source": [
    "# 手写交叉熵\n",
    "# 模拟一组预测概率（已过 Sigmoid）和真实标签\n",
    "y_pred = torch.tensor([0.668, 0.231, 0.881], requires_grad=True)  # 模型预测的概率\n",
    "y_true = torch.tensor([1.0, 0.0, 1.0])  # 标签\n",
    "n = y_pred.shape[0]\n",
    "epsilon = 1e-7\n",
    "loss = -torch.mean(y_true * torch.log(y_pred + epsilon) + (1 - y_true) * torch.log(1 - y_pred + epsilon))\n",
    "print(\"Loss: \", loss.item())\n",
    "\n",
    "gradient_manual = -(y_true/y_pred - (1 - y_true)/(1 - y_pred)) / n\n",
    "print(\"Gradient: \", gradient_manual)\n",
    "\n",
    "# 自动求导\n",
    "# loss.backward()\n",
    "# print(\"Autograd gradient:\", y_pred.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "486a72ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross Entropy Loss: 0.2644655704498291\n",
      "gradient_manual:  tensor([-0.1106,  0.0772, -0.0397], grad_fn=<DivBackward0>)\n",
      "logits 的梯度： tensor([-0.1106,  0.0772, -0.0397])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设模型输出的是 logits（未经过 Sigmoid）\n",
    "logits = torch.tensor([0.7, -1.2, 2.0], requires_grad=True)  # shape: [batch_size]\n",
    "labels = torch.tensor([1.0, 0.0, 1.0])  # shape: [batch_size]\n",
    "epsilon = 0\n",
    "loss = -torch.mean(y_true * torch.log(torch.sigmoid(logits) + epsilon) + (1 - y_true) * torch.log(1 - torch.sigmoid(logits) + epsilon))\n",
    "\n",
    "print(\"Binary Cross Entropy Loss:\", loss.item())\n",
    "n = logits.shape[0]\n",
    "y_pred = torch.sigmoid(logits)\n",
    "gradient_manual = - (y_true - y_pred)/n\n",
    "\n",
    "print(\"gradient_manual: \", gradient_manual)\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "print(\"logits 的梯度：\", logits.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d7dfef",
   "metadata": {},
   "source": [
    "## 多分类\n",
    "Loss function，我们不妨只考虑单个样本  \n",
    "\n",
    "有C类\n",
    "\n",
    "z = [z1, z2, z3,...,Z_C]\n",
    "\n",
    "y = [y1, y2, y3,...,y_C]\n",
    "\n",
    "$y_i = \\frac{e^{z_i}}{\\sum_{j \\in C}e^{z_j}}$\n",
    "\n",
    "$Loss = - \\log y_k$ k为真实标签\n",
    "\n",
    "$\\frac{d Loss}{d y_k} = - \\frac{1}{y_k}$\n",
    "\n",
    "$\\frac{\\partial y_k}{\\partial z_k} = \\frac{e^{z_k}\\sum_{j \\in C}e^{z_j} - e^{z_k}e^{z_k}}{(\\sum_{j \\in C}e^{z_j})^2} = y_k(1 - y_k)$\n",
    "\n",
    "$\\frac{\\partial y_k}{\\partial z_i} = -\\frac{e^{z_k}e^{z_i}}{(\\sum_{j \\in C}e^{z_j})^2} = y_ky_i$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial z_k} = y_k - 1$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial z_i} = y_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbbe5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss: 2.1160569190979004\n",
      "logits 的梯度: tensor([[ 0.3557, -0.4206,  0.0650],\n",
      "        [ 0.0414,  0.4129, -0.4543]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 模拟 logits（未归一化分数）和标签\n",
    "logits = torch.tensor([[2.0, 0.5, 0.3],\n",
    "                       [0.2, 2.5, 0.3]], requires_grad=True)  # shape: [batch_size, num_classes]\n",
    "labels = torch.tensor([1, 2])  # 每个样本的类别索引\n",
    "\n",
    "# 创建损失函数对象\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(\"Cross Entropy Loss:\", loss.item())\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "print(\"logits 的梯度:\", logits.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc2dd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.3891,  1.6487,  1.3499],\n",
      "        [ 1.2214, 12.1825,  1.3499]], grad_fn=<ExpBackward0>)\n",
      "tensor([10.3876, 14.7538], grad_fn=<SumBackward1>)\n",
      "tensor([[10.3876],\n",
      "        [14.7538]], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.7113, 0.1587, 0.1299],\n",
       "        [0.0828, 0.8257, 0.0915]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_softmax(logits):\n",
    "    print(torch.exp(logits))\n",
    "    print(torch.sum(torch.exp(logits), dim=1))\n",
    "    print(torch.sum(torch.exp(logits), dim=1, keepdim=True))\n",
    "    return torch.exp(logits)/torch.sum(torch.exp(logits), dim=1, keepdim=True)\n",
    "\n",
    "logits = torch.tensor([[2.0, 0.5, 0.3],\n",
    "                       [0.2, 2.5, 0.3]], requires_grad=True)  # shape: [batch_size, num_classes]\n",
    "my_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5051e94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my softmax:  tensor([[0.7113, 0.1587, 0.1299],\n",
      "        [0.0828, 0.8257, 0.0915]], grad_fn=<DivBackward0>)\n",
      "softmax:  tensor([[0.7113, 0.1587, 0.1299],\n",
      "        [0.0828, 0.8257, 0.0915]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([2])\n",
      "torch.Size([2, 1])\n",
      "tensor([0.1587, 0.0915], grad_fn=<SqueezeBackward0>)\n",
      "tensor(2.1161, grad_fn=<NegBackward0>)\n",
      "tensor([0.5000, 0.3000], grad_fn=<SqueezeBackward0>)\n",
      "tensor([2.3406, 2.6915], grad_fn=<LogBackward0>)\n",
      "tensor(2.1161, grad_fn=<MeanBackward0>)\n",
      "mannual gradient:  tensor([[ 0.3557, -0.4206,  0.0650],\n",
      "        [ 0.0414,  0.4129, -0.4543]], grad_fn=<DivBackward0>)\n",
      "auto gradient:  tensor([[ 0.3557, -0.4206,  0.0650],\n",
      "        [ 0.0414,  0.4129, -0.4543]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 手写多分类\n",
    "\n",
    "# 模拟 logits（未归一化分数）和标签\n",
    "logits = torch.tensor([[2.0, 0.5, 0.3],\n",
    "                       [0.2, 2.5, 0.3]], requires_grad=True)  # shape: [batch_size, num_classes]\n",
    "labels = torch.tensor([1, 2])  # 每个样本的类别索引\n",
    "n = logits.shape[0]\n",
    "\n",
    "softmax = torch.softmax(logits, dim=1)\n",
    "mysoft = my_softmax(logits)\n",
    "print(\"my softmax: \", mysoft)\n",
    "print(\"softmax: \", softmax)\n",
    "print(labels.shape)\n",
    "print(labels.unsqueeze(1).shape)\n",
    "\n",
    "# softmax\n",
    "probs = softmax.gather(dim=1, index = labels.unsqueeze(1)).squeeze()\n",
    "print(probs)\n",
    "loss = -torch.mean(torch.log(probs))\n",
    "print(loss)\n",
    "\n",
    "\n",
    "# 再拆开\n",
    "index_loss = logits.gather(dim=1, index=labels.unsqueeze(1)).squeeze()\n",
    "other_loss = torch.log(torch.sum(torch.exp(logits), dim = 1))\n",
    "print(index_loss)\n",
    "print(other_loss)\n",
    "loss = torch.mean(other_loss - index_loss)\n",
    "print(loss)\n",
    "\n",
    "# 手动梯度\n",
    "\n",
    "gradient_manual = softmax\n",
    "gradient_manual[range(len(labels)), labels] -= 1\n",
    "gradient_manual /= n\n",
    "print(\"mannual gradient: \", gradient_manual)\n",
    "loss.backward()\n",
    "print(\"auto gradient: \", logits.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb415e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mappo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
