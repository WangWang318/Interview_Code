{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c3bf24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2.2951815128326416\n",
      "Loss Manual:  2.2951815128326416\n",
      "Gradient:  tensor([-0.5089, -1.4948, 47.6189])\n",
      "Gradient Manual tensor([-0.5089, -1.4948,  0.3357], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "y_pred = torch.tensor([0.655, 0.223, 0.993], requires_grad=True)\n",
    "y_label = torch.tensor([1, 1, 0], dtype=torch.float32)\n",
    "n = y_pred.shape[0]\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "loss = criterion(y_pred, y_label)\n",
    "\n",
    "loss_manual = -torch.mean(y_label * torch.log(y_pred) + (1 - y_label)*torch.log(1 - y_pred))\n",
    "gradient_manual = - (y_label / y_pred - (1 - y_label) / y_pred) /n\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"Loss: \", loss.item())\n",
    "print(\"Loss Manual: \", loss_manual.item())\n",
    "print(\"Gradient: \", y_pred.grad)\n",
    "print(\"Gradient Manual\", gradient_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a442bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax:  tensor([[0.4506, 0.3021, 0.2473],\n",
      "        [0.2966, 0.4003, 0.3031],\n",
      "        [0.2687, 0.4063, 0.3250]], grad_fn=<SoftmaxBackward0>)\n",
      "Mannual softmax:  tensor([[0.4506, 0.3021, 0.2473],\n",
      "        [0.2966, 0.4003, 0.3031],\n",
      "        [0.2687, 0.4063, 0.3250]], grad_fn=<DivBackward0>)\n",
      "Probability:  tensor([0.4506, 0.4003, 0.3250], grad_fn=<SqueezeBackward0>)\n",
      "softmax:  tensor([[-0.1831,  0.1007,  0.0824],\n",
      "        [ 0.0989, -0.1999,  0.1010],\n",
      "        [ 0.0896,  0.1354, -0.2250]], grad_fn=<DivBackward0>)\n",
      "Loss:  0.9455661773681641\n",
      "Gradient:  tensor([[-0.1831,  0.1007,  0.0824],\n",
      "        [ 0.0989, -0.1999,  0.1010],\n",
      "        [ 0.0896,  0.1354, -0.2250]])\n",
      "gradient_manual tensor([[-0.1831,  0.1007,  0.0824],\n",
      "        [ 0.0989, -0.1999,  0.1010],\n",
      "        [ 0.0896,  0.1354, -0.2250]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def mannual_softmax(input):\n",
    "    return torch.exp(input)/torch.sum(torch.exp(input), dim=1, keepdim=True)\n",
    "\n",
    "y_logits = torch.tensor([[0.8, 0.4, 0.2],\n",
    "                         [0.2, 0.5, 0.222],\n",
    "                         [0.11, 0.5234, 0.3]], requires_grad=True)\n",
    "\n",
    "y_label = torch.tensor([0, 1, 2])\n",
    "n = y_logits.shape[0]\n",
    "\n",
    "soft_max = torch.softmax(y_logits, dim=1)\n",
    "mannual_soft_max = mannual_softmax(y_logits)\n",
    "print(\"softmax: \", soft_max)\n",
    "print(\"Mannual softmax: \", mannual_soft_max)\n",
    "\n",
    "y_prob = soft_max.gather(dim=1, index=y_label.unsqueeze(1)).squeeze()\n",
    "print(\"Probability: \", y_prob)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "loss_manual = - torch.mean(torch.log(y_prob))\n",
    "\n",
    "gradient_manual = soft_max\n",
    "gradient_manual[range(n), y_label] -= 1\n",
    "gradient_manual /= n\n",
    "\n",
    "\n",
    "loss = criterion(y_logits, y_label)\n",
    "loss.backward()\n",
    "print(\"softmax: \", soft_max)\n",
    "print(\"Loss: \", loss.item())\n",
    "# print(\"Loss Manual: \", loss_manual)\n",
    "\n",
    "print(\"Gradient: \", y_logits.grad)\n",
    "print(\"gradient_manual\", gradient_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2224879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: 构造 one-hot 标签矩阵\n",
    "y_one_hot = torch.zeros_like(y_logits)  # shape: [batch, num_classes]\n",
    "y_one_hot.scatter_(1, y_label.unsqueeze(1), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155de117",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "scatter(): Expected self.dtype to be equal to src.dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m]])\n\u001b[1;32m      3\u001b[0m src \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m], [\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m40\u001b[39m]])\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(target)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: scatter(): Expected self.dtype to be equal to src.dtype"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1be34b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([[-7.8559e-08, -2.8900e-08, -1.1750e-08]])\n"
     ]
    }
   ],
   "source": [
    "# 检验计算图是否被污染\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[2.0, 1.0, 0.1]], requires_grad=True)\n",
    "y = torch.softmax(x, dim=1)\n",
    "# y[0, 0] -= 1  # 修改计算图中间变量！\n",
    "\n",
    "z = y.sum()\n",
    "z.backward()\n",
    "print(\"x.grad:\", x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b015c59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([100., 100., 100.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# # 克隆出来\n",
    "# x_clone = x.clone()\n",
    "\n",
    "# # 改变 x_clone 的值（仍然有 grad_fn）\n",
    "# x_clone[0] = x_clone[0] * 100  # 不 in-place，所以合法\n",
    "\n",
    "# # 用 clone 参与计算\n",
    "# y = x_clone.sum()\n",
    "\n",
    "y = torch.sum(x[0] * 100)\n",
    "y.backward()\n",
    "\n",
    "print(\"x.grad:\", x.grad)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mappo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
