{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e357722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.3468,  0.3706, -0.2624,  0.2491],\n",
      "         [ 0.3442,  0.3662, -0.2685,  0.2541],\n",
      "         [ 0.3465,  0.3706, -0.2625,  0.2492]],\n",
      "\n",
      "        [[ 0.4045,  0.2085, -0.3606,  0.2901],\n",
      "         [ 0.4062,  0.2053, -0.3626,  0.2926],\n",
      "         [ 0.4031,  0.2112, -0.3585,  0.2880]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[0.3342, 0.3263, 0.3395],\n",
      "         [0.3281, 0.3408, 0.3310],\n",
      "         [0.3327, 0.3262, 0.3411]],\n",
      "\n",
      "        [[0.3364, 0.3301, 0.3335],\n",
      "         [0.3289, 0.3408, 0.3303],\n",
      "         [0.3396, 0.3216, 0.3388]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# attention\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "d_model = 6\n",
    "d_k = 4\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"input shape: \", x.shape)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_k)\n",
    "        self.W_k = nn.Linear(d_model, d_k)\n",
    "        self.W_v = nn.Linear(d_model, d_k)\n",
    "\n",
    "    def forward(self, input):   # [B, S, d_model]\n",
    "        Q = self.W_q(input)     # [B, S, d_k]\n",
    "        K = self.W_k(input)     # [B, S, d_k]\n",
    "        V = self.W_v(input)     # [B, S, d_k]\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)     # [B, S, S]\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(weights, V)\n",
    "\n",
    "        return output, weights\n",
    "\n",
    "attention_layer = SelfAttention(d_model, d_k)\n",
    "\n",
    "output, weights = attention_layer(x)\n",
    "\n",
    "print(output)\n",
    "\n",
    "\n",
    "\n",
    "print(weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([2, 3, 6])\n",
      "tensor([[ True, False, False],\n",
      "        [ True,  True, False],\n",
      "        [ True,  True,  True]])\n",
      "socres: tensor([[[0.0288,   -inf,   -inf],\n",
      "         [0.3989, 0.3232,   -inf],\n",
      "         [0.1370, 0.1270, 0.1868]],\n",
      "\n",
      "        [[0.4134,   -inf,   -inf],\n",
      "         [0.1390, 0.0844,   -inf],\n",
      "         [0.1108, 0.0873, 0.0983]]], grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[-0.2365, -0.4558, -0.2359, -0.0013],\n",
      "         [-0.2038, -0.4263, -0.2420, -0.1248],\n",
      "         [-0.1586, -0.4470, -0.2827, -0.1519]],\n",
      "\n",
      "        [[-0.3216, -0.5624, -0.3853, -0.1773],\n",
      "         [-0.2795, -0.5421, -0.4857, -0.2161],\n",
      "         [-0.2835, -0.5046, -0.4398, -0.1511]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[1.0000, 0.0000, 0.0000],\n",
      "         [0.5189, 0.4811, 0.0000],\n",
      "         [0.3288, 0.3256, 0.3456]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000],\n",
      "         [0.5137, 0.4863, 0.0000],\n",
      "         [0.3373, 0.3295, 0.3332]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# mask\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "d_model = 6\n",
    "d_k = 4\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"input shape: \", x.shape)\n",
    "\n",
    "def causal_mask(seq_len):\n",
    "    return torch.tril(torch.ones(seq_len, seq_len)).bool()  # [L, L]\n",
    "\n",
    "mask = causal_mask(seq_len)  # [L, L]\n",
    "print(mask)\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_k)\n",
    "        self.W_k = nn.Linear(d_model, d_k)\n",
    "        self.W_v = nn.Linear(d_model, d_k)\n",
    "\n",
    "    def forward(self, input, attn_mask=None):   # [B, S, d_model]\n",
    "        Q = self.W_q(input)     # [B, S, d_k]\n",
    "        K = self.W_k(input)     # [B, S, d_k]\n",
    "        V = self.W_v(input)     # [B, S, d_k]\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)     # [B, S, S]\n",
    "\n",
    "        if attn_mask != None:\n",
    "            scores = scores.masked_fill(attn_mask==0, float('-inf'))\n",
    "            print(\"socres:\", scores)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(weights, V)\n",
    "\n",
    "        return output, weights\n",
    "\n",
    "attention_layer = SelfAttention(d_model, d_k)\n",
    "\n",
    "output, weights = attention_layer(x, mask)\n",
    "\n",
    "print(\"output: \", output)\n",
    "\n",
    "print(\"weights: \", weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c5581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_head\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, attn_mask=None):\n",
    "        B, S, E = input.size()\n",
    "        Q = self.W_q(input).view(B, S, self.num_heads, self.d_k).transpose(1, 2) # [B, S, E] -> [B, S, n, d_k] -> [B, n, S, d_k]\n",
    "        K = self.W_k(input).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(input).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.d_k ** 0.5)    # [B, n, S, S]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask==0, float('-inf'))\n",
    "        \n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        output = torch.matmul(weights, V)   # [B, n, S, d_k]\n",
    "\n",
    "        output = output.transpose(1,2).contiguous().view(B, S, self.d_model) # [B, S, d_model]\n",
    "\n",
    "        output = self.dropout(self.W_o(output))\n",
    "    \n",
    "\n",
    "        return output, weights\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.ff(input)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.attention_layer = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask):   # [B, S, E]\n",
    "        attn_output, _ = self.attention_layer(x, attn_mask)   # [B, S, E]\n",
    "        x = self.norm1(x + self.dropout_1(attn_output))\n",
    "\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout_2(ff_output))\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e666a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([2, 3, 6])\n",
      "tensor([[ True, False, False],\n",
      "        [ True,  True, False],\n",
      "        [ True,  True,  True]])\n",
      "output:  tensor([[[-0.1975, -0.0850, -0.1854,  0.1128,  0.1626,  0.0000],\n",
      "         [-0.1770, -0.1456, -0.3501,  0.0945,  0.1552,  0.7728],\n",
      "         [-0.1121, -0.1763, -0.4046,  0.1056,  0.1187,  0.8214]],\n",
      "\n",
      "        [[-0.0216, -0.1186, -0.6042,  0.1005,  0.1399,  0.9678],\n",
      "         [-0.0302, -0.1038, -0.4704,  0.1165,  0.1151,  0.8514],\n",
      "         [-0.0285, -0.0904, -0.5128,  0.1219,  0.1011,  0.8995]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "weights:  tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4953, 0.5047, 0.0000],\n",
      "          [0.3346, 0.3354, 0.3300]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4939, 0.5061, 0.0000],\n",
      "          [0.3271, 0.3307, 0.3422]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4991, 0.5009, 0.0000],\n",
      "          [0.3393, 0.3337, 0.3270]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4998, 0.5002, 0.0000],\n",
      "          [0.3451, 0.3252, 0.3297]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "d_model = 6\n",
    "num_heads = 2\n",
    "d_k = 4\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"input shape: \", x.shape)\n",
    "\n",
    "def causal_mask(seq_len):\n",
    "    return torch.tril(torch.ones(seq_len, seq_len)).bool()  # [L, L]\n",
    "\n",
    "mask = causal_mask(seq_len)  # [L, L]\n",
    "print(mask)\n",
    "\n",
    "multi_attention_layer = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "output, weights = multi_attention_layer(x, mask)\n",
    "\n",
    "print(\"output: \", output)\n",
    "\n",
    "print(\"weights: \", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "073a181b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1341,  0.3455,  2.0616,  0.7853],\n",
      "         [ 0.5331, -0.0438,  0.1966, -1.7723],\n",
      "         [ 1.6760, -0.1478,  0.9186,  1.5295]],\n",
      "\n",
      "        [[-0.0292,  0.2269,  1.4519, -0.1068],\n",
      "         [-1.3315, -2.5104, -0.3037,  0.4920],\n",
      "         [ 0.3821, -1.8754, -0.1821, -0.5776]]])\n",
      "causal mask:  tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "output:  tensor([[[-0.2300, -0.8331,  1.6908, -0.6278],\n",
      "         [ 1.0556, -0.3513,  0.7690, -1.4734],\n",
      "         [ 1.0836, -1.5699,  0.5917, -0.1054]],\n",
      "\n",
      "        [[ 0.2094, -0.5744,  1.5166, -1.1516],\n",
      "         [ 0.0330, -1.6348,  0.9485,  0.6533],\n",
      "         [ 1.1670, -1.3824,  0.7010, -0.4856]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "attm_score:  tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5217, 0.4783, 0.0000],\n",
      "          [0.3498, 0.3204, 0.3298]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4930, 0.5070, 0.0000],\n",
      "          [0.3249, 0.3154, 0.3596]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4969, 0.5031, 0.0000],\n",
      "          [0.3278, 0.3437, 0.3285]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5201, 0.4799, 0.0000],\n",
      "          [0.3234, 0.3473, 0.3293]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model//n_heads\n",
    "\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads.\"\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        B, S, E = x.shape # [B, S, d_model]\n",
    "\n",
    "        Q = self.W_q(x).view(B, S, self.n_heads, self.d_k).transpose(1,2)   # [B, S, d_model] -> [B, S, n_heads, d_k] -> [B, n_heads, S, d_k]\n",
    "        K = self.W_k(x).view(B, S, self.n_heads, self.d_k).transpose(1,2)\n",
    "        V = self.W_v(x).view(B, S, self.n_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.d_k**0.5)  # [B, n_heads, S, d_k] * [B, n_heads, d_k, S] -> [B, n_heads, S, S]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask == 0, float('-inf'))\n",
    "            \n",
    "        att_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        output = torch.matmul(att_weights, V)   # [B, n_heads, S, S] * [B, n_heads, S, d_k] -> [B, n_heads, S, d_k]\n",
    "        output = output.transpose(1,2).contiguous() # [B, n_heads, S, d_k] -> [B, S, n_heads, d_k] contiguous的作用是让output再内存上连续，接下来可以用view\n",
    "        output = output.view(B, S, E)   # [B, S, n_heads, d_k] -> [B, S, d_model]\n",
    "\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output, weights\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "    \n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output, _ = self.self_attn(x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "\n",
    "        ffn_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "\n",
    "        return x, _\n",
    "\n",
    "    \n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "n_heads = 2\n",
    "d_ff = 8\n",
    "x = torch.randn(batch_size, seq_len, d_model)   # \n",
    "print(x)\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "print(\"causal mask: \", causal_mask)\n",
    "transformer_layer = TransformerLayer(d_model=d_model, n_heads=n_heads, d_ff=d_ff)\n",
    "\n",
    "output, attn_score = transformer_layer(x, causal_mask)\n",
    "\n",
    "print(\"output: \", output)\n",
    "print(\"attm_score: \", attn_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "973d4b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2095,  1.9240,  0.3394,  1.5518,  2.2554,  2.3029, -0.2029,\n",
      "          -0.5393],\n",
      "         [ 1.9780, -0.5150, -2.1081,  1.3318,  0.4494,  0.2211, -2.2074,\n",
      "           0.5719],\n",
      "         [ 0.8145, -0.9975,  0.5430,  0.1208, -0.1950, -0.3852,  0.0622,\n",
      "          -0.3854]],\n",
      "\n",
      "        [[-0.3090, -1.9795,  0.6845,  1.5346, -1.5454,  0.8559,  1.2202,\n",
      "          -1.2872],\n",
      "         [ 0.8155,  0.3763,  1.3528, -1.0328,  0.1628, -0.3964,  1.3299,\n",
      "          -0.7186],\n",
      "         [-0.2249,  0.3653,  1.0429,  0.9632, -1.9671, -0.3240, -0.3628,\n",
      "           0.4104]]])\n",
      "causal mask:  tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "output:  tensor([[[ 7.9113e-02, -2.6621e-01,  2.1510e-01, -1.3914e-01,  1.0455e+00,\n",
      "          -8.1278e-01,  7.2820e-02, -5.4629e-01],\n",
      "         [ 3.5965e-01, -2.3883e-01,  3.4103e-01,  1.6718e-01,  3.9212e-01,\n",
      "          -5.2973e-01, -2.8044e-02,  1.4039e-02],\n",
      "         [ 2.4854e-01,  2.9665e-02,  1.7839e-01,  5.8713e-02,  3.9517e-01,\n",
      "          -2.8310e-01,  1.2582e-01, -6.5580e-02]],\n",
      "\n",
      "        [[ 3.7253e-01,  1.2937e-01,  1.7102e-01, -4.9632e-03,  3.6553e-01,\n",
      "          -2.2843e-01,  3.9816e-04, -2.5885e-01],\n",
      "         [ 4.1217e-01,  1.9645e-01,  2.4052e-02, -9.7868e-02,  4.9113e-01,\n",
      "          -1.7093e-01, -1.5510e-01, -1.9343e-01],\n",
      "         [ 3.0165e-01,  1.3419e-01,  4.6060e-02, -1.2450e-01,  6.1549e-01,\n",
      "          -2.6923e-01, -1.1648e-01, -2.1734e-01]]], grad_fn=<ViewBackward0>)\n",
      "attm_score:  tensor([[[[ 0.2225,    -inf,    -inf],\n",
      "          [-0.8357, -0.3153,    -inf],\n",
      "          [-0.4920, -0.2621,  0.0456]],\n",
      "\n",
      "         [[-0.6382,    -inf,    -inf],\n",
      "          [-0.1997,  0.0194,    -inf],\n",
      "          [ 0.1874,  0.0112,  0.0221]],\n",
      "\n",
      "         [[-2.6816,    -inf,    -inf],\n",
      "          [-2.2035, -0.3750,    -inf],\n",
      "          [ 0.7562,  0.1076, -0.0610]],\n",
      "\n",
      "         [[ 0.3102,    -inf,    -inf],\n",
      "          [ 3.7083,  0.5808,    -inf],\n",
      "          [ 0.1374,  0.0980, -0.1190]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1362,    -inf,    -inf],\n",
      "          [ 0.2360,  0.0913,    -inf],\n",
      "          [ 0.1793,  0.1705,  0.0373]],\n",
      "\n",
      "         [[ 0.4203,    -inf,    -inf],\n",
      "          [ 0.1044, -0.0430,    -inf],\n",
      "          [ 0.7486,  0.5275, -0.0494]],\n",
      "\n",
      "         [[-0.7490,    -inf,    -inf],\n",
      "          [ 0.1083, -0.1071,    -inf],\n",
      "          [-0.5395,  0.4094, -0.5803]],\n",
      "\n",
      "         [[ 0.2282,    -inf,    -inf],\n",
      "          [ 0.2209, -0.2753,    -inf],\n",
      "          [-0.0408, -0.0485, -0.0946]]]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# MQA, GQA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_kv_heads):   # kv头的数量\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.n_rep = n_heads // n_kv_heads  \n",
    "\n",
    "        self.W_q = nn.Linear(d_model, self.d_k * n_heads)\n",
    "        self.W_k = nn.Linear(d_model, self.d_k * n_kv_heads)\n",
    "        self.W_v = nn.Linear(d_model, self.d_k * n_kv_heads)\n",
    "\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, S, E = x.shape\n",
    "        Q = self.W_q(x).view(B, S, self.n_heads, self.d_k).transpose(1,2) # [B, S, d_model] -> [B, S, n_heads, d_k] -> [B, n_heads, S, d_k]\n",
    "        K = self.W_k(x).view(B, S, self.n_kv_heads, self.d_k).transpose(1,2)  # [B, S, d_model] -> [B, S, n_kv_heads, d_k] -> [B, n_kv_heads, S, d_k]\n",
    "        V = self.W_v(x).view(B, S, self.n_kv_heads, self.d_k).transpose(1,2)    # [B, S, d_model] -> [B, S, n_kv_heads, d_k] -> [B, n_kv_heads, S, d_k]\n",
    "\n",
    "        # 复制KV头以匹配Q的头数\n",
    "        if self.n_rep > 1:\n",
    "            K = K.repeat_interleave(self.n_rep, dim = 1)    # [B, n_kv_heads, S, d_k] -> # [B, n_kv_heads * n_rep = n_heads, S, d_k]\n",
    "            V = V.repeat_interleave(self.n_rep, dim = 1)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)   # [B, n_heads, S, S]\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(weights, V)   # [B, n_heads, S, S] * [B, n_heads, S, d_k] -> [B, n_heads, S, d_k]\n",
    "\n",
    "        output = output.transpose(1,2).contiguous().view(B, S, E)   # [B, S, d_model]\n",
    "\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, scores\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d_model = 8\n",
    "n_heads = 4\n",
    "n_kv_heads = 2\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)   # \n",
    "print(x)\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "print(\"causal mask: \", causal_mask)\n",
    "transformer_layer = GroupedQueryAttention(d_model=d_model, n_heads=n_heads, n_kv_heads=n_kv_heads)\n",
    "\n",
    "output, attn_score = transformer_layer(x, causal_mask)\n",
    "\n",
    "print(\"output: \", output)\n",
    "print(\"attm_score: \", attn_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7af3abb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Prefill ---\n",
      "Output: tensor([[[ 0.0478,  0.1587,  0.3308, -0.2052],\n",
      "         [ 0.2319,  0.0765,  0.2512, -0.1282],\n",
      "         [ 0.2933,  0.0798,  0.3320, -0.1028]]], grad_fn=<ViewBackward0>)\n",
      "Cache K shape torch.Size([1, 2, 3, 2])\n",
      "\n",
      "--- Phase 2: Decoding Step 1 ---\n",
      "torch.Size([1, 1, 4])\n",
      "Output tensor([[[ 0.0478,  0.1587,  0.3308, -0.2052]]], grad_fn=<ViewBackward0>)\n",
      "Cache K shape: torch.Size([1, 2, 1, 2])\n",
      "torch.Size([1, 1, 4])\n",
      "Output tensor([[[ 0.2319,  0.0765,  0.2512, -0.1282]]], grad_fn=<ViewBackward0>)\n",
      "Cache K shape: torch.Size([1, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 带KV cache的MHA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CachedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        assert d_model % n_heads == 0\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, past_kv=None, mask=None):\n",
    "        \n",
    "        B, S, E = x.shape    # 推理时，通常B = 1, S = 1\n",
    "\n",
    "        # 1. 投影新输入\n",
    "        # Q, K, V shape: [Batch, n_heads, seq_len (通常为1), d_k]\n",
    "        Q = self.W_q(x).view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            K = torch.cat([past_k, K], dim=2)\n",
    "            V = torch.cat([past_v, V], dim=2)\n",
    "\n",
    "        current_kv = (K, V)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.d_k ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            # 在 Decoding 阶段，如果 Q_len 为 1，通常不需要 causal mask，\n",
    "            # 因为它天生只能看到过去。但在 Prefill 阶段需要。\n",
    "            # 这里为了通用性，简单处理：如果 mask 尺寸匹配就应用。\n",
    "            if mask.shape[-1] == scores.shape[-1] and mask.shape[-2] == scores.shape[-2]:\n",
    "                 scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)    \n",
    "\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(B, S, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output, current_kv\n",
    "    \n",
    "# --- 设置 ---\n",
    "d_model = 4\n",
    "n_heads = 2\n",
    "\n",
    "model = CachedMultiHeadAttention(d_model, n_heads)\n",
    "model.eval() # 推理模式\n",
    "\n",
    "# 阶段 1: Prefill (处理输入的 Prompt)\n",
    "prompt_seq_len = 3\n",
    "input_prompt = torch.randn(1, prompt_seq_len, d_model) # [1, 3, 4]\n",
    "causal_mask = torch.tril(torch.ones(prompt_seq_len, prompt_seq_len)).bool()\n",
    "\n",
    "print(\"--- Phase 1: Prefill ---\")\n",
    "\n",
    "output, kv_cache = model(input_prompt, mask=causal_mask, past_kv=None)\n",
    "\n",
    "print(\"Output:\", output) # [1, 3, 4]\n",
    "print(\"Cache K shape\", kv_cache[0].shape) # [1, 2, 3, 2] (Batch, n_heads, Seq, d_k)\n",
    "\n",
    "\n",
    "# 阶段 2: 想去check一下mask的逻辑有没有写错\n",
    "print(\"\\n--- Phase 2: Decoding Step 1 ---\")\n",
    "input_new_token = input_prompt[:, :1, :] # 取最后一个时间步, shape: [1, 1, 4]\n",
    "print(input_new_token.shape)\n",
    "\n",
    "output_1, kv_cache = model(input_new_token, mask=None, past_kv=None)\n",
    "\n",
    "print(\"Output\", output_1) # [1, 1, 4] -> 生成了 1 个新 token 的表示\n",
    "print(\"Cache K shape:\", kv_cache[0].shape) \n",
    "\n",
    "\n",
    "input_next_token = input_prompt[:, 1:2, :] # 假设它就是下一个输入, shape [1, 1, 4]\n",
    "print(input_new_token.shape)\n",
    "output_2, kv_cache = model(input_next_token, mask=None, past_kv=kv_cache)\n",
    "\n",
    "print(\"Output\", output_2) # [1, 1, 4] -> 生成了 1 个新 token 的表示\n",
    "print(\"Cache K shape:\", kv_cache[0].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa2ba288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Mode A: Training (Parallel) ---\n",
      "Training output shape: torch.Size([2, 3, 4])\n",
      "\n",
      "--- Mode B: Inference (Sequential) ---\n",
      "Prefill output shape: torch.Size([1, 3, 4])\n",
      "Initial KV Cache len: 3\n",
      "Decoding step 1...\n",
      "KV cache shape:  torch.Size([1, 2, 4, 2])\n",
      "Decoding step 2...\n",
      "KV cache shape:  torch.Size([1, 2, 5, 2])\n",
      "Decoding step 3...\n",
      "KV cache shape:  torch.Size([1, 2, 6, 2])\n",
      "Final KV Cache len: 6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UnifiedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask = None, past_kv=None, use_cache=False):\n",
    "        B, S_q, _ = x.shape # S_q 是 Query 的序列长度\n",
    "\n",
    "        Q = self.W_q(x).view(B, S_q, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, S_q, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, S_q, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            K = torch.cat([past_k, K], dim=2)\n",
    "            V = torch.cat([past_v, V], dim=2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "\n",
    "        # 4. Mask 处理 (自动适配两种模式)\n",
    "        if mask is not None:\n",
    "            # 我们需要确保 mask 的形状能和 scores 广播匹配\n",
    "            # 训练时 scores 是 [B, H, S, S], mask 通常是 [S, S] -> 完美匹配\n",
    "            # 推理时 scores 是 [B, H, 1, S_past+1], mask 通常为 None (或需要特殊处理 padding)\n",
    "            # 这里做一个简单的防呆兼容\n",
    "            if mask.shape[-2] == scores.shape[-2] and mask.shape[-1] == scores.shape[-1]:\n",
    "                 scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(B, S_q, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        if use_cache:\n",
    "            current_kv = (K, V)\n",
    "            return output, current_kv\n",
    "        else:\n",
    "            return output, None\n",
    "\n",
    "\n",
    "d_model = 4\n",
    "n_heads = 2\n",
    "seq_len = 3\n",
    "model = UnifiedMultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "\n",
    "# 模式 A: 训练模式 (Training Mode)\n",
    "# 特点：并行、无 Cache、必须有 Mask\n",
    "print(\"--- Mode A: Training (Parallel) ---\")\n",
    "train_input = torch.randn(2, seq_len, d_model) # Batch=2\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len)).bool()\n",
    "\n",
    "# 调用方式：不传 past_kv，use_cache=False\n",
    "train_output, _ = model(train_input, mask=causal_mask, use_cache=False)\n",
    "\n",
    "print(\"Training output shape:\", train_output.shape) # [2, 5, 8]\n",
    "\n",
    "\n",
    "# 模式 B: 推理模式 (Inference Mode)\n",
    "# 特点：串行、使用 Cache\n",
    "print(\"\\n--- Mode B: Inference (Sequential) ---\")\n",
    "model.eval() # 切换到评估模式 (虽然这里 MHA 没有 dropout/BN，但好习惯要有)\n",
    "\n",
    "# 1. Prefill 阶段 (处理 Prompt \"I like eating\")\n",
    "prompt = torch.randn(1, 3, d_model) # 假设 Prompt 长度为 3\n",
    "prompt_mask = torch.tril(torch.ones(3, 3)).bool()\n",
    "\n",
    "# 调用方式：开启 use_cache=True\n",
    "output_prefill, kv_cache = model(prompt, mask=prompt_mask, use_cache=True)\n",
    "print(\"Prefill output shape:\", output_prefill.shape) # [1, 3, 8]\n",
    "print(\"Initial KV Cache len:\", kv_cache[0].shape[2]) # 应该是 3\n",
    "\n",
    "# 2. Decoding 循环阶段\n",
    "next_token = output_prefill[:, -1:, :] # 假装这是采样出的新 token\n",
    "\n",
    "for i in range(3): # 再生成 3 个词\n",
    "    print(f\"Decoding step {i+1}...\")\n",
    "    # 调用方式：传入上一步的 next_token 和 kv_cache\n",
    "    # 注意：这里 mask=None，因为 Q长度=1，它默认可以看所有过去的 K\n",
    "    output_step, kv_cache = model(next_token, past_kv=kv_cache, use_cache=True, mask=None)\n",
    "    \n",
    "    # 假装采样\n",
    "    next_token = output_step \n",
    "    print(\"KV cache shape: \", kv_cache[0].shape)\n",
    "    \n",
    "print(\"Final KV Cache len:\", kv_cache[0].shape[2]) # 3(prefill) + 3(decoding) = 6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mappo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
