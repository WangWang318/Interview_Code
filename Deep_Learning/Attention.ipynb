{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e357722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.3468,  0.3706, -0.2624,  0.2491],\n",
      "         [ 0.3442,  0.3662, -0.2685,  0.2541],\n",
      "         [ 0.3465,  0.3706, -0.2625,  0.2492]],\n",
      "\n",
      "        [[ 0.4045,  0.2085, -0.3606,  0.2901],\n",
      "         [ 0.4062,  0.2053, -0.3626,  0.2926],\n",
      "         [ 0.4031,  0.2112, -0.3585,  0.2880]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[0.3342, 0.3263, 0.3395],\n",
      "         [0.3281, 0.3408, 0.3310],\n",
      "         [0.3327, 0.3262, 0.3411]],\n",
      "\n",
      "        [[0.3364, 0.3301, 0.3335],\n",
      "         [0.3289, 0.3408, 0.3303],\n",
      "         [0.3396, 0.3216, 0.3388]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# attention\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "d_model = 6\n",
    "d_k = 4\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"input shape: \", x.shape)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_k)\n",
    "        self.W_k = nn.Linear(d_model, d_k)\n",
    "        self.W_v = nn.Linear(d_model, d_k)\n",
    "\n",
    "    def forward(self, input):   # [B, S, d_model]\n",
    "        Q = self.W_q(input)     # [B, S, d_k]\n",
    "        K = self.W_k(input)     # [B, S, d_k]\n",
    "        V = self.W_v(input)     # [B, S, d_k]\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)     # [B, S, S]\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(weights, V)\n",
    "\n",
    "        return output, weights\n",
    "\n",
    "attention_layer = SelfAttention(d_model, d_k)\n",
    "\n",
    "output, weights = attention_layer(x)\n",
    "\n",
    "print(output)\n",
    "\n",
    "\n",
    "\n",
    "print(weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([2, 3, 6])\n",
      "tensor([[ True, False, False],\n",
      "        [ True,  True, False],\n",
      "        [ True,  True,  True]])\n",
      "socres: tensor([[[0.0288,   -inf,   -inf],\n",
      "         [0.3989, 0.3232,   -inf],\n",
      "         [0.1370, 0.1270, 0.1868]],\n",
      "\n",
      "        [[0.4134,   -inf,   -inf],\n",
      "         [0.1390, 0.0844,   -inf],\n",
      "         [0.1108, 0.0873, 0.0983]]], grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[-0.2365, -0.4558, -0.2359, -0.0013],\n",
      "         [-0.2038, -0.4263, -0.2420, -0.1248],\n",
      "         [-0.1586, -0.4470, -0.2827, -0.1519]],\n",
      "\n",
      "        [[-0.3216, -0.5624, -0.3853, -0.1773],\n",
      "         [-0.2795, -0.5421, -0.4857, -0.2161],\n",
      "         [-0.2835, -0.5046, -0.4398, -0.1511]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[1.0000, 0.0000, 0.0000],\n",
      "         [0.5189, 0.4811, 0.0000],\n",
      "         [0.3288, 0.3256, 0.3456]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000],\n",
      "         [0.5137, 0.4863, 0.0000],\n",
      "         [0.3373, 0.3295, 0.3332]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# mask\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "d_model = 6\n",
    "d_k = 4\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"input shape: \", x.shape)\n",
    "\n",
    "def causal_mask(seq_len):\n",
    "    return torch.tril(torch.ones(seq_len, seq_len)).bool()  # [L, L]\n",
    "\n",
    "mask = causal_mask(seq_len)  # [L, L]\n",
    "print(mask)\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_k)\n",
    "        self.W_k = nn.Linear(d_model, d_k)\n",
    "        self.W_v = nn.Linear(d_model, d_k)\n",
    "\n",
    "    def forward(self, input, attn_mask=None):   # [B, S, d_model]\n",
    "        Q = self.W_q(input)     # [B, S, d_k]\n",
    "        K = self.W_k(input)     # [B, S, d_k]\n",
    "        V = self.W_v(input)     # [B, S, d_k]\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)     # [B, S, S]\n",
    "\n",
    "        if attn_mask != None:\n",
    "            scores = scores.masked_fill(attn_mask==0, float('-inf'))\n",
    "            print(\"socres:\", scores)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(weights, V)\n",
    "\n",
    "        return output, weights\n",
    "\n",
    "attention_layer = SelfAttention(d_model, d_k)\n",
    "\n",
    "output, weights = attention_layer(x, mask)\n",
    "\n",
    "print(\"output: \", output)\n",
    "\n",
    "print(\"weights: \", weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c5581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_head\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, attn_mask=None):\n",
    "        B, S, E = input.size()\n",
    "        Q = self.W_q(input).view(B, S, self.num_heads, self.d_k).transpose(1, 2) # [B, S, E] -> [B, S, n, d_k] -> [B, n, S, d_k]\n",
    "        K = self.W_k(input).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(input).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.d_k ** 0.5)    # [B, n, S, S]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask==0, float('-inf'))\n",
    "        \n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        output = torch.matmul(weights, V)   # [B, n, S, d_k]\n",
    "\n",
    "        output = output.transpose(1,2).contiguous().view(B, S, self.d_model) # [B, S, d_model]\n",
    "\n",
    "        output = self.dropout(self.W_o(output))\n",
    "    \n",
    "\n",
    "        return output, weights\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.ff(input)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.attention_layer = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask):   # [B, S, E]\n",
    "        attn_output, _ = self.attention_layer(x, attn_mask)   # [B, S, E]\n",
    "        x = self.norm1(x + self.dropout_1(attn_output))\n",
    "\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout_2(ff_output))\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e666a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([2, 3, 6])\n",
      "tensor([[ True, False, False],\n",
      "        [ True,  True, False],\n",
      "        [ True,  True,  True]])\n",
      "output:  tensor([[[-0.1975, -0.0850, -0.1854,  0.1128,  0.1626,  0.0000],\n",
      "         [-0.1770, -0.1456, -0.3501,  0.0945,  0.1552,  0.7728],\n",
      "         [-0.1121, -0.1763, -0.4046,  0.1056,  0.1187,  0.8214]],\n",
      "\n",
      "        [[-0.0216, -0.1186, -0.6042,  0.1005,  0.1399,  0.9678],\n",
      "         [-0.0302, -0.1038, -0.4704,  0.1165,  0.1151,  0.8514],\n",
      "         [-0.0285, -0.0904, -0.5128,  0.1219,  0.1011,  0.8995]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "weights:  tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4953, 0.5047, 0.0000],\n",
      "          [0.3346, 0.3354, 0.3300]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4939, 0.5061, 0.0000],\n",
      "          [0.3271, 0.3307, 0.3422]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4991, 0.5009, 0.0000],\n",
      "          [0.3393, 0.3337, 0.3270]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4998, 0.5002, 0.0000],\n",
      "          [0.3451, 0.3252, 0.3297]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "d_model = 6\n",
    "num_heads = 2\n",
    "d_k = 4\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"input shape: \", x.shape)\n",
    "\n",
    "def causal_mask(seq_len):\n",
    "    return torch.tril(torch.ones(seq_len, seq_len)).bool()  # [L, L]\n",
    "\n",
    "mask = causal_mask(seq_len)  # [L, L]\n",
    "print(mask)\n",
    "\n",
    "multi_attention_layer = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "output, weights = multi_attention_layer(x, mask)\n",
    "\n",
    "print(\"output: \", output)\n",
    "\n",
    "print(\"weights: \", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a181b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.2497, -0.5584,  0.6232, -1.8979],\n",
      "         [-0.9936, -1.4758,  0.3628,  1.2568],\n",
      "         [-1.2396, -0.7735, -0.2151, -1.6438]],\n",
      "\n",
      "        [[ 0.6813,  0.9366, -1.1800,  0.3557],\n",
      "         [-1.3757, -1.8015,  1.0607, -0.7222],\n",
      "         [-0.0020,  0.0931, -1.1953, -0.5659]]])\n",
      "causal mask:  tensor([[ True, False, False],\n",
      "        [ True,  True, False],\n",
      "        [ True,  True,  True]])\n",
      "tensor([[[-0.0836,  0.0493,  0.7319,  0.0353],\n",
      "         [-0.4484, -0.5112,  0.6098, -0.1604],\n",
      "         [-0.2701, -0.2868,  0.6053, -0.0994]],\n",
      "\n",
      "        [[ 0.4637, -0.2665, -0.8256, -0.4612],\n",
      "         [ 0.0328, -0.3005, -0.0077, -0.2496],\n",
      "         [-0.2698, -0.3900,  0.3720, -0.0704]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5217, 0.4783, 0.0000],\n",
      "          [0.3498, 0.3204, 0.3298]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4930, 0.5070, 0.0000],\n",
      "          [0.3249, 0.3154, 0.3596]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4969, 0.5031, 0.0000],\n",
      "          [0.3278, 0.3437, 0.3285]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5201, 0.4799, 0.0000],\n",
      "          [0.3234, 0.3473, 0.3293]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model//n_heads\n",
    "\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads.\"\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        B, S, E = x.shape # [B, S, d_model]\n",
    "\n",
    "        Q = self.W_q(x).view(B, S, self.n_heads, self.d_k).transpose(1,2)   # [B, S, d_model] -> [B, S, n_heads, d_k] -> [B, n_heads, S, d_k]\n",
    "        K = self.W_k(x).view(B, S, self.n_heads, self.d_k).transpose(1,2)\n",
    "        V = self.W_v(x).view(B, S, self.n_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.d_k**0.5)  # [B, n_heads, S, d_k] * [B, n_heads, d_k, S] -> [B, n_heads, S, S]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask == 0, float('-inf'))\n",
    "            \n",
    "        att_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        output = torch.matmul(att_weights, V)   # [B, n_heads, S, S] * [B, n_heads, S, d_k] -> [B, n_heads, S, d_k]\n",
    "        output = output.transpose(1,2).contiguous() # [B, n_heads, S, d_k] -> [B, S, n_heads, d_k] contiguous的作用是让output再内存上连续，接下来可以用view\n",
    "        output = output.view(B, S, E)   # [B, S, n_heads, d_k] -> [B, S, d_model]\n",
    "\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output, weights\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            \n",
    "\n",
    "        )\n",
    "    \n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "n_heads = 2\n",
    "x = torch.randn(batch_size, seq_len, d_model)   # \n",
    "print(x)\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len)).bool()\n",
    "print(\"causal mask: \", causal_mask)\n",
    "mha_layer = MultiHeadAttention(d_model=d_model, n_heads=n_heads)\n",
    "\n",
    "output, attn_score = mha_layer(x, causal_mask)\n",
    "print(output)\n",
    "\n",
    "print(attn_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mappo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
