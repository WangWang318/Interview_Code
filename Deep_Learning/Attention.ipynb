{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e357722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([2, 3, 6])\n",
      "tensor([[[-0.2794, -0.7736, -0.1859, -0.9512],\n",
      "         [-0.2818, -0.7787, -0.1934, -0.9619],\n",
      "         [-0.2805, -0.7759, -0.1893, -0.9560]],\n",
      "\n",
      "        [[-0.1900, -0.9145,  0.0406, -0.8660],\n",
      "         [-0.1910, -0.9132,  0.0398, -0.8660],\n",
      "         [-0.1894, -0.9151,  0.0412, -0.8660]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[0.3370, 0.3252, 0.3378],\n",
      "         [0.3176, 0.3494, 0.3330],\n",
      "         [0.3284, 0.3362, 0.3355]],\n",
      "\n",
      "        [[0.3333, 0.3413, 0.3254],\n",
      "         [0.3310, 0.3479, 0.3211],\n",
      "         [0.3356, 0.3369, 0.3276]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# attention\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "d_model = 6\n",
    "d_k = 4\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"input shape: \", x.shape)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_k)\n",
    "        self.W_k = nn.Linear(d_model, d_k)\n",
    "        self.W_v = nn.Linear(d_model, d_k)\n",
    "\n",
    "    def forward(self, input):   # [B, S, d_model]\n",
    "        Q = self.W_q(input)     # [B, S, d_k]\n",
    "        K = self.W_k(input)     # [B, S, d_k]\n",
    "        V = self.W_v(input)     # [B, S, d_k]\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)     # [B, S, S]\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(weights, V)\n",
    "\n",
    "        return output, weights\n",
    "\n",
    "attention_layer = SelfAttention(d_model, d_k)\n",
    "\n",
    "output, weights = attention_layer(x)\n",
    "\n",
    "print(output)\n",
    "\n",
    "print(weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([2, 3, 6])\n",
      "tensor([[ True, False, False],\n",
      "        [ True,  True, False],\n",
      "        [ True,  True,  True]])\n",
      "socres: tensor([[[0.0288,   -inf,   -inf],\n",
      "         [0.3989, 0.3232,   -inf],\n",
      "         [0.1370, 0.1270, 0.1868]],\n",
      "\n",
      "        [[0.4134,   -inf,   -inf],\n",
      "         [0.1390, 0.0844,   -inf],\n",
      "         [0.1108, 0.0873, 0.0983]]], grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[-0.2365, -0.4558, -0.2359, -0.0013],\n",
      "         [-0.2038, -0.4263, -0.2420, -0.1248],\n",
      "         [-0.1586, -0.4470, -0.2827, -0.1519]],\n",
      "\n",
      "        [[-0.3216, -0.5624, -0.3853, -0.1773],\n",
      "         [-0.2795, -0.5421, -0.4857, -0.2161],\n",
      "         [-0.2835, -0.5046, -0.4398, -0.1511]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[1.0000, 0.0000, 0.0000],\n",
      "         [0.5189, 0.4811, 0.0000],\n",
      "         [0.3288, 0.3256, 0.3456]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000],\n",
      "         [0.5137, 0.4863, 0.0000],\n",
      "         [0.3373, 0.3295, 0.3332]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# mask\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "d_model = 6\n",
    "d_k = 4\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"input shape: \", x.shape)\n",
    "\n",
    "def causal_mask(seq_len):\n",
    "    return torch.tril(torch.ones(seq_len, seq_len)).bool()  # [L, L]\n",
    "\n",
    "mask = causal_mask(seq_len)  # [L, L]\n",
    "print(mask)\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_k)\n",
    "        self.W_k = nn.Linear(d_model, d_k)\n",
    "        self.W_v = nn.Linear(d_model, d_k)\n",
    "\n",
    "    def forward(self, input, attn_mask=None):   # [B, S, d_model]\n",
    "        Q = self.W_q(input)     # [B, S, d_k]\n",
    "        K = self.W_k(input)     # [B, S, d_k]\n",
    "        V = self.W_v(input)     # [B, S, d_k]\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)     # [B, S, S]\n",
    "\n",
    "        if attn_mask != None:\n",
    "            scores = scores.masked_fill(attn_mask==0, float('-inf'))\n",
    "            print(\"socres:\", scores)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(weights, V)\n",
    "\n",
    "        return output, weights\n",
    "\n",
    "attention_layer = SelfAttention(d_model, d_k)\n",
    "\n",
    "output, weights = attention_layer(x, mask)\n",
    "\n",
    "print(\"output: \", output)\n",
    "\n",
    "print(\"weights: \", weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "911c5581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_head\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, attn_mask=None):\n",
    "        B, S, E = input.size()\n",
    "        Q = self.W_q(input).view(B, S, self.num_heads, self.d_k).transpose(1, 2) # [B, S, E] -> [B, S, n, d_k] -> [B, n, S, d_k]\n",
    "        K = self.W_k(input).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(input).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.d_k ** 0.5)    # [B, n, S, S]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask==0, float('-inf'))\n",
    "        \n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        output = torch.matmul(weights, V)   # [B, n, S, d_k]\n",
    "\n",
    "        output = output.transpose(1,2).contiguous().view(B, S, self.d_model) # [B, S, d_model]\n",
    "\n",
    "        output = self.dropout(self.W_o(output))\n",
    "    \n",
    "\n",
    "        return output, weights\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.ff(input)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.attention_layer = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask):   # [B, S, E]\n",
    "        attn_output, _ = self.attention_layer(x, attn_mask)   # [B, S, E]\n",
    "        x = self.norm1(x + self.dropout_1(attn_output))\n",
    "\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout_2(ff_output))\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e666a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([2, 3, 6])\n",
      "tensor([[ True, False, False],\n",
      "        [ True,  True, False],\n",
      "        [ True,  True,  True]])\n",
      "output:  tensor([[[-0.1975, -0.0850, -0.1854,  0.1128,  0.1626,  0.0000],\n",
      "         [-0.1770, -0.1456, -0.3501,  0.0945,  0.1552,  0.7728],\n",
      "         [-0.1121, -0.1763, -0.4046,  0.1056,  0.1187,  0.8214]],\n",
      "\n",
      "        [[-0.0216, -0.1186, -0.6042,  0.1005,  0.1399,  0.9678],\n",
      "         [-0.0302, -0.1038, -0.4704,  0.1165,  0.1151,  0.8514],\n",
      "         [-0.0285, -0.0904, -0.5128,  0.1219,  0.1011,  0.8995]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "weights:  tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4953, 0.5047, 0.0000],\n",
      "          [0.3346, 0.3354, 0.3300]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4939, 0.5061, 0.0000],\n",
      "          [0.3271, 0.3307, 0.3422]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4991, 0.5009, 0.0000],\n",
      "          [0.3393, 0.3337, 0.3270]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4998, 0.5002, 0.0000],\n",
      "          [0.3451, 0.3252, 0.3297]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "d_model = 6\n",
    "num_heads = 2\n",
    "d_k = 4\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"input shape: \", x.shape)\n",
    "\n",
    "def causal_mask(seq_len):\n",
    "    return torch.tril(torch.ones(seq_len, seq_len)).bool()  # [L, L]\n",
    "\n",
    "mask = causal_mask(seq_len)  # [L, L]\n",
    "print(mask)\n",
    "\n",
    "multi_attention_layer = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "output, weights = multi_attention_layer(x, mask)\n",
    "\n",
    "print(\"output: \", output)\n",
    "\n",
    "print(\"weights: \", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a181b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mappo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
